<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Gemini Streaming Translation Test</title>
    <style>
        * { box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
            background: #1a1a2e;
            color: #eee;
        }
        h1 { color: #00d4ff; margin-bottom: 5px; }
        .subtitle { color: #888; margin-bottom: 20px; }
        .panel {
            background: #16213e;
            border-radius: 10px;
            padding: 20px;
            margin-bottom: 20px;
        }
        .status {
            display: flex;
            align-items: center;
            gap: 10px;
            margin-bottom: 15px;
        }
        .status-dot {
            width: 12px;
            height: 12px;
            border-radius: 50%;
            background: #666;
        }
        .status-dot.connected { background: #00ff88; }
        .status-dot.streaming { background: #ffd700; animation: pulse 0.5s infinite; }
        .status-dot.speaking { background: #ff6b6b; animation: pulse 0.3s infinite; }
        @keyframes pulse { 0%, 100% { opacity: 1; } 50% { opacity: 0.5; } }

        button {
            background: #00d4ff;
            color: #000;
            border: none;
            padding: 12px 24px;
            border-radius: 6px;
            font-size: 16px;
            cursor: pointer;
            margin-right: 10px;
            margin-bottom: 10px;
        }
        button:hover { background: #00b8e6; }
        button:disabled { background: #444; color: #888; cursor: not-allowed; }
        button.stop { background: #ff6b6b; }
        button.stop:hover { background: #ff5252; }

        .config {
            display: grid;
            grid-template-columns: 1fr 1fr 1fr 1fr;
            gap: 15px;
            margin-bottom: 15px;
        }
        label { display: block; margin-bottom: 5px; color: #aaa; }
        select, input {
            width: 100%;
            padding: 10px;
            border-radius: 6px;
            border: 1px solid #333;
            background: #0f0f23;
            color: #fff;
            font-size: 14px;
        }

        .mode-badge {
            display: inline-block;
            padding: 4px 12px;
            border-radius: 12px;
            font-size: 12px;
            font-weight: bold;
            margin-left: 10px;
            background: #00ff88;
            color: #000;
        }

        .log {
            background: #0f0f23;
            border-radius: 6px;
            padding: 15px;
            height: 250px;
            overflow-y: auto;
            font-family: 'Consolas', 'Monaco', monospace;
            font-size: 13px;
            line-height: 1.5;
        }
        .log-entry { margin-bottom: 5px; }
        .log-entry.event { color: #00d4ff; }
        .log-entry.source { color: #ffd700; }
        .log-entry.translation { color: #00ff88; }
        .log-entry.error { color: #ff6b6b; }
        .log-entry.audio { color: #888; }

        .translation-display {
            background: #0f0f23;
            border-radius: 6px;
            padding: 20px;
            min-height: 100px;
        }
        .translation-display .source {
            color: #ffd700;
            font-size: 18px;
            margin-bottom: 10px;
        }
        .translation-display .translated {
            color: #00ff88;
            font-size: 20px;
            font-weight: bold;
        }
        .translation-display .placeholder {
            color: #666;
            font-style: italic;
        }
        .streaming-indicator {
            display: none;
            color: #ffd700;
            font-size: 12px;
            margin-top: 10px;
            animation: blink 1s infinite;
        }
        @keyframes blink { 0%, 100% { opacity: 1; } 50% { opacity: 0.3; } }

        .mute-indicator {
            display: none;
            padding: 4px 12px;
            border-radius: 12px;
            font-size: 12px;
            font-weight: bold;
            margin-left: 10px;
            background: #ff6b6b;
            color: #fff;
            animation: pulse 0.5s infinite;
        }
    </style>
</head>
<body>
    <h1>Gemini Streaming Translation</h1>
    <p class="subtitle">Real-time translation with persistent Gemini Live session</p>

    <div class="panel">
        <div class="status">
            <div class="status-dot" id="statusDot"></div>
            <span id="statusText">Disconnected</span>
            <span class="mode-badge" id="modeBadge" style="display: none;">STREAMING</span>
            <span class="mute-indicator" id="muteIndicator">MIC MUTED</span>
        </div>

        <div class="config">
            <div>
                <label>Server URL</label>
                <input type="text" id="serverUrl" value="ws://localhost:8001/ws/translate">
            </div>
            <div>
                <label>API Key (optional)</label>
                <input type="password" id="apiKey" placeholder="Leave empty if not required">
            </div>
            <div>
                <label>Target Language</label>
                <select id="targetLang">
                    <option value="it">Italian</option>
                    <option value="en">English</option>
                    <option value="de">German</option>
                    <option value="es">Spanish</option>
                    <option value="fr">French</option>
                </select>
            </div>
            <div>
                <label>Gemini Voice</label>
                <select id="geminiVoice">
                    <option value="Kore">Kore (Female)</option>
                    <option value="Aoede">Aoede (Female)</option>
                    <option value="Charon">Charon (Male)</option>
                    <option value="Fenrir">Fenrir (Male)</option>
                    <option value="Puck">Puck (Male)</option>
                </select>
            </div>
        </div>

        <button id="startBtn" onclick="start()">Start Streaming</button>
        <button id="endTurnBtn" onclick="endTurn()" disabled style="display: none;">End Turn (Manual)</button>
        <button id="stopBtn" onclick="stop()" disabled class="stop">Stop</button>
    </div>

    <div class="panel">
        <h3>Live Translation</h3>
        <div class="translation-display">
            <div class="source" id="sourceText"></div>
            <div class="translated" id="translatedText"></div>
            <div class="placeholder" id="placeholder">Speak into your microphone...</div>
            <div class="streaming-indicator" id="streamingIndicator">Listening and translating in real-time...</div>
        </div>
    </div>

    <div class="panel">
        <h3>Event Log</h3>
        <div class="log" id="log"></div>
    </div>

    <script>
        let ws = null;
        let audioContext = null;
        let mediaStream = null;
        let processor = null;
        let isRecording = false;
        let audioQueue = [];
        let isPlaying = false;
        let isMuted = false;  // Mute mic during audio playback
        let unmuteTimeout = null;

        // Manual VAD State Machine
        // IDLE: waiting for speech, not sending audio to server
        // ACTIVE: speech detected, sent activity_start, sending audio chunks
        // WAIT_COMPLETE: sent activity_end, waiting for turn_complete, NOT sending audio
        let turnState = 'IDLE';

        // VAD settings (ottimizzate per VoIP/telefonia)
        const VAD_SPEECH_THRESHOLD = 0.012;   // RMS threshold to detect speech start (lower for VoIP)
        const VAD_SILENCE_THRESHOLD = 0.006;  // RMS threshold for silence (lower for telephony)
        const VAD_SILENCE_DURATION_MS = 350;  // Trigger translation after 350ms silence
        const MIN_TURN_DURATION_MS = 900;     // Don't close turn if less than 900ms of speech
        const MAX_TURN_MS = 5000;             // Safety limit at 5s, NOT a fixed timer
        const AUTO_RESTART_THRESHOLD = 0.009; // RMS threshold for auto-restart (coerente con speech)

        // Buffer e Overlap settings
        const OVERLAP_MS = 250;
        const OVERLAP_SAMPLES = Math.floor(16000 * (OVERLAP_MS / 1000)); // 4000 samples
        const OVERLAP_BYTES = OVERLAP_SAMPLES * 2; // PCM16 = 8000 bytes
        const PENDING_MAX_BYTES = 16000 * 2 * 2; // ~2s di PCM16 mono @16kHz = 64KB

        let silenceStartTime = null;
        let speechStartTime = null;           // When the current turn started
        let sampleRateLogged = false;         // Log sample rate once
        let lastRms = 0;                      // Store last RMS for auto-restart in handleMessage
        let overlapTail = null;               // Last 250ms of audio for overlap
        let pendingAudioChunks = [];          // Buffer audio during WAIT_COMPLETE
        let pendingBytes = 0;                 // Track pending buffer size

        function setTurnState(newState) {
            const oldState = turnState;
            turnState = newState;
            log(`Turn state: ${oldState} → ${newState}`, 'event');
        }

        const statusDot = document.getElementById('statusDot');
        const statusText = document.getElementById('statusText');
        const startBtn = document.getElementById('startBtn');
        const endTurnBtn = document.getElementById('endTurnBtn');
        const stopBtn = document.getElementById('stopBtn');
        const logDiv = document.getElementById('log');
        const sourceText = document.getElementById('sourceText');
        const translatedText = document.getElementById('translatedText');
        const placeholder = document.getElementById('placeholder');
        const modeBadge = document.getElementById('modeBadge');
        const streamingIndicator = document.getElementById('streamingIndicator');
        const muteIndicator = document.getElementById('muteIndicator');

        function log(message, type = '') {
            const entry = document.createElement('div');
            entry.className = 'log-entry ' + type;
            entry.textContent = `[${new Date().toLocaleTimeString()}] ${message}`;
            logDiv.appendChild(entry);
            logDiv.scrollTop = logDiv.scrollHeight;
        }

        // Normalize BCP-47 language codes to simple 2-letter codes
        // Gemini doesn't accept extended codes like 'es-ES', only 'es'
        function normalizeLanguageCode(code) {
            if (!code || code === 'auto') return code;
            return code.split('-')[0].split('_')[0].toLowerCase();
        }

        function setStatus(status, state = 'idle') {
            statusText.textContent = status;
            statusDot.className = 'status-dot';
            if (status.includes('Connected')) statusDot.classList.add('connected');
            if (state === 'streaming') statusDot.classList.add('streaming');
            if (state === 'speaking') statusDot.classList.add('speaking');
        }

        async function start() {
            try {
                // Get microphone access
                log('Requesting microphone access...');
                mediaStream = await navigator.mediaDevices.getUserMedia({
                    audio: {
                        sampleRate: 16000,
                        channelCount: 1,
                        echoCancellation: true,
                        noiseSuppression: true
                    }
                });

                // Create audio context
                audioContext = new AudioContext({ sampleRate: 16000 });

                // Connect to WebSocket
                let serverUrl = document.getElementById('serverUrl').value;
                const apiKey = document.getElementById('apiKey').value;

                // Add API key if provided
                if (apiKey) {
                    const separator = serverUrl.includes('?') ? '&' : '?';
                    serverUrl = `${serverUrl}${separator}key=${encodeURIComponent(apiKey)}`;
                    log(`Connecting to ${document.getElementById('serverUrl').value} (with API key)...`);
                } else {
                    log(`Connecting to ${serverUrl}...`);
                }

                ws = new WebSocket(serverUrl);
                ws.binaryType = 'arraybuffer';

                ws.onopen = () => {
                    log('WebSocket connected', 'event');
                    setStatus('Connected - Configuring...');
                };

                ws.onmessage = async (event) => {
                    if (event.data instanceof ArrayBuffer) {
                        // Audio data as ArrayBuffer - queue for playback
                        const audioData = new Int16Array(event.data);
                        log(`Received ${audioData.length} audio samples (ArrayBuffer)`, 'audio');
                        queueAudioPlayback(audioData);
                    } else if (event.data instanceof Blob) {
                        // Audio data as Blob - convert to ArrayBuffer first
                        const arrayBuffer = await event.data.arrayBuffer();
                        const audioData = new Int16Array(arrayBuffer);
                        log(`Received ${audioData.length} audio samples (Blob)`, 'audio');
                        queueAudioPlayback(audioData);
                    } else if (typeof event.data === 'string') {
                        // JSON message
                        const data = JSON.parse(event.data);
                        handleMessage(data);
                    } else {
                        log(`Unknown message type: ${typeof event.data}`, 'error');
                    }
                };

                ws.onclose = () => {
                    log('WebSocket disconnected', 'error');
                    setStatus('Disconnected');
                    stopRecording();
                    modeBadge.style.display = 'none';
                    streamingIndicator.style.display = 'none';
                    endTurnBtn.disabled = true;
                };

                ws.onerror = (err) => {
                    log('WebSocket error: ' + err.message, 'error');
                };

                startBtn.disabled = true;
                endTurnBtn.disabled = false;
                stopBtn.disabled = false;

            } catch (err) {
                log('Error: ' + err.message, 'error');
                console.error(err);
            }
        }

        function endTurn() {
            if (ws && ws.readyState === WebSocket.OPEN && turnState === 'ACTIVE') {
                const turnDuration = speechStartTime ? Date.now() - speechStartTime : 0;
                log(`Manual: Sending activity_end (turn ${turnDuration}ms)...`, 'event');
                ws.send(JSON.stringify({ type: 'activity_end' }));
                setTurnState('WAIT_COMPLETE');
                silenceStartTime = null;
                speechStartTime = null;
            }
        }

        function handleMessage(data) {
            switch (data.type) {
                case 'connected':
                    log(`Server: ${data.server}, Model: ${data.model}`, 'event');
                    log(`Voice: ${data.voice}, Streaming: ${data.streaming_supported}`, 'event');
                    setStatus('Connected - Configuring...');

                    // Configure language (normalize codes for Gemini compatibility)
                    const targetLang = normalizeLanguageCode(document.getElementById('targetLang').value);
                    ws.send(JSON.stringify({
                        type: 'configure',
                        source_lang: 'auto',
                        target_lang: targetLang
                    }));
                    break;

                case 'configured':
                    log(`Language: ${data.source_lang} -> ${data.target_lang}`, 'event');

                    // Enable streaming mode
                    ws.send(JSON.stringify({ type: 'set_streaming', enabled: true }));
                    break;

                case 'streaming_enabled':
                    if (data.enabled) {
                        log('Streaming mode enabled - Manual VAD with client-side silence detection', 'event');
                        setStatus('Connected - Streaming', 'streaming');
                        modeBadge.style.display = 'inline-block';
                        streamingIndicator.style.display = 'block';
                        placeholder.style.display = 'none';
                        // Reset state machine
                        turnState = 'IDLE';
                        silenceStartTime = null;
                        speechStartTime = null;
                        sampleRateLogged = false;
                        // Show manual End Turn button
                        endTurnBtn.style.display = 'inline-block';
                        endTurnBtn.disabled = false;
                        // Start recording
                        startRecording();
                    } else {
                        log('Streaming mode disabled', 'event');
                        modeBadge.style.display = 'none';
                        streamingIndicator.style.display = 'none';
                        endTurnBtn.style.display = 'none';
                    }
                    break;

                case 'streaming_session_ready':
                    log(`Streaming session ready: ${data.source_lang} -> ${data.target_lang}`, 'event');
                    break;

                case 'source_text':
                    // Streaming source text (incremental)
                    if (data.text) {
                        sourceText.textContent = data.text;
                        log(`Source: "${data.text}"`, 'source');
                        setStatus('Connected - Speaking...', 'speaking');
                    }
                    break;

                case 'translated_text':
                    // Streaming translated text (incremental)
                    if (data.text) {
                        translatedText.textContent = data.text;
                        log(`Translation: "${data.text}"`, 'translation');
                    }
                    break;

                case 'model_text':
                    // TEXT response from model (for debugging when AUDIO+TEXT modalities enabled)
                    if (data.text) {
                        log(`Model TEXT: "${data.text}"`, 'translation');
                    }
                    break;

                case 'model_turn_started':
                    log('Gemini speaking (your audio is buffered)', 'event');
                    setStatus('Connected - Translating...', 'speaking');
                    break;

                case 'turn_complete':
                    log('Turn complete - checking for auto-restart', 'event');
                    setStatus('Connected - Streaming', 'streaming');

                    if (lastRms > AUTO_RESTART_THRESHOLD) {
                        log(`Auto-restart (RMS: ${lastRms.toFixed(4)}) - sending activity_start`, 'event');
                        ws.send(JSON.stringify({ type: 'activity_start' }));

                        // Invia overlap per continuità lessicale
                        if (overlapTail) {
                            log(`Sending overlap: ${overlapTail.byteLength} bytes`, 'audio');
                            ws.send(overlapTail);
                            overlapTail = null;
                        }

                        // Flush pending audio (bufferizzato durante WAIT_COMPLETE)
                        if (pendingAudioChunks.length) {
                            log(`Flushing buffered audio: ${pendingAudioChunks.length} chunks (${pendingBytes} bytes)`, 'event');
                            for (const buf of pendingAudioChunks) {
                                ws.send(buf);
                            }
                            pendingAudioChunks = [];
                            pendingBytes = 0;
                        }

                        setTurnState('ACTIVE');
                        speechStartTime = Date.now();
                        silenceStartTime = null;
                    } else {
                        // No speech detected - go to IDLE and wait for next speech
                        setTurnState('IDLE');
                        silenceStartTime = null;
                        speechStartTime = null;
                        // Svuota buffer se nessuno parla
                        pendingAudioChunks = [];
                        pendingBytes = 0;
                    }
                    break;

                case 'end_of_turn_sent':
                    log('End of turn acknowledged - waiting for translation', 'event');
                    break;

                case 'activity_start_sent':
                    log('Activity start acknowledged - listening...', 'event');
                    break;

                case 'activity_end_sent':
                    log('Activity end acknowledged - translating...', 'event');
                    break;

                case 'translation':
                    // Full translation (buffer mode)
                    log(`Source: "${data.source_text}"`, 'source');
                    log(`Translation: "${data.translated_text}"`, 'translation');
                    sourceText.textContent = data.source_text;
                    translatedText.textContent = data.translated_text;
                    break;

                case 'error':
                    log(`Error: ${data.message}`, 'error');
                    break;

                default:
                    log(`Event: ${data.type}`, 'event');
            }
        }

        function startRecording() {
            if (isRecording) return;

            const source = audioContext.createMediaStreamSource(mediaStream);

            // Use ScriptProcessorNode for audio capture
            processor = audioContext.createScriptProcessor(4096, 1, 1);

            processor.onaudioprocess = (e) => {
                if (!isRecording || !ws || ws.readyState !== WebSocket.OPEN) return;

                // Skip sending audio when muted (during playback)
                if (isMuted) return;

                const inputData = e.inputBuffer.getChannelData(0);

                // Log sample rate once for debugging
                if (!sampleRateLogged) {
                    log(`Input buffer sampleRate: ${e.inputBuffer.sampleRate}`, 'event');
                    log(`AudioContext sampleRate: ${audioContext.sampleRate}`, 'event');
                    sampleRateLogged = true;
                }

                // Calculate RMS energy for voice activity detection
                let sum = 0;
                for (let i = 0; i < inputData.length; i++) {
                    sum += inputData[i] * inputData[i];
                }
                const rms = Math.sqrt(sum / inputData.length);
                lastRms = rms;  // Store for auto-restart check in handleMessage

                const now = Date.now();

                // STATE MACHINE LOGIC
                if (turnState === 'IDLE') {
                    // Waiting for speech - check if speech starts
                    if (rms > VAD_SPEECH_THRESHOLD) {
                        // Speech detected! Send activity_start and transition to ACTIVE
                        log(`Speech detected (RMS: ${rms.toFixed(4)}) - sending activity_start`, 'event');
                        ws.send(JSON.stringify({ type: 'activity_start' }));

                        // Invia overlap per continuità lessicale (dal turno precedente)
                        if (overlapTail) {
                            log(`Sending overlap: ${overlapTail.byteLength} bytes`, 'audio');
                            ws.send(overlapTail);
                            overlapTail = null;
                        }

                        setTurnState('ACTIVE');
                        speechStartTime = now;  // Track when this turn started
                        silenceStartTime = null;
                        // DON'T return - continue to send this first chunk!
                    } else {
                        // In IDLE state with no speech, do NOT send audio chunks
                        return;
                    }
                }

                if (turnState === 'ACTIVE') {
                    const turnDuration = now - speechStartTime;

                    // MAX_TURN come SAFETY: chiudi solo se near-silence
                    // NON troncare in piena sillaba!
                    if (turnDuration >= MAX_TURN_MS && rms < VAD_SILENCE_THRESHOLD) {
                        log(`Max turn ${turnDuration}ms near-silence - sending activity_end`, 'event');
                        ws.send(JSON.stringify({ type: 'activity_end' }));
                        setTurnState('WAIT_COMPLETE');
                        silenceStartTime = null;
                        return;
                    }

                    // Chiusura preferita: su silenzio vero (350ms)
                    if (rms > VAD_SILENCE_THRESHOLD) {
                        // Still speaking - reset silence timer
                        silenceStartTime = null;
                    } else {
                        // Silence detected - start timer if not already started
                        if (!silenceStartTime) {
                            silenceStartTime = now;
                        }

                        // Check if silence duration exceeded threshold
                        const silenceDuration = now - silenceStartTime;

                        // Close turn on natural pauses
                        if (silenceDuration >= VAD_SILENCE_DURATION_MS && turnDuration >= MIN_TURN_DURATION_MS) {
                            log(`Silence ${silenceDuration}ms (turn ${turnDuration}ms) - sending activity_end`, 'event');
                            ws.send(JSON.stringify({ type: 'activity_end' }));
                            setTurnState('WAIT_COMPLETE');
                            silenceStartTime = null;
                            return;
                        }
                    }

                    // Convert Float32 to Int16 and send audio
                    const int16Data = new Int16Array(inputData.length);
                    for (let i = 0; i < inputData.length; i++) {
                        const s = Math.max(-1, Math.min(1, inputData[i]));
                        int16Data[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
                    }

                    // Salva tail per overlap (ultimi 250ms per continuità lessicale)
                    const bytes = new Uint8Array(int16Data.buffer);
                    if (bytes.byteLength >= OVERLAP_BYTES) {
                        overlapTail = bytes.slice(bytes.byteLength - OVERLAP_BYTES).buffer;
                    } else {
                        overlapTail = int16Data.buffer;
                    }

                    ws.send(int16Data.buffer);
                    return;
                }

                if (turnState === 'WAIT_COMPLETE') {
                    // NON droppare! Continua a calcolare RMS e BUFFERIZZA audio
                    // lastRms è già aggiornato sopra, quindi auto-restart userà RMS corrente

                    // Converti e bufferizza
                    const int16Data = new Int16Array(inputData.length);
                    for (let i = 0; i < inputData.length; i++) {
                        const s = Math.max(-1, Math.min(1, inputData[i]));
                        int16Data[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
                    }

                    // Limita la memoria: tieni solo gli ultimi ~2s
                    pendingAudioChunks.push(int16Data.buffer);
                    pendingBytes += int16Data.byteLength;
                    while (pendingBytes > PENDING_MAX_BYTES) {
                        const removed = pendingAudioChunks.shift();
                        pendingBytes -= removed.byteLength;
                    }
                    return;
                }
            };

            source.connect(processor);
            processor.connect(audioContext.destination);

            isRecording = true;
            log('Recording started - speak into your microphone', 'event');
        }

        function stopRecording() {
            isRecording = false;
            if (processor) {
                processor.disconnect();
                processor = null;
            }
        }

        function queueAudioPlayback(int16Data) {
            // Full duplex mode: don't mute microphone during playback
            // Echo cancellation should handle feedback
            // This allows speaking while translation is playing
            log(`Queueing ${int16Data.length} samples for playback`, 'audio');
            audioQueue.push(int16Data);
            if (!isPlaying) {
                playNextChunk();
            }
        }

        async function playNextChunk() {
            if (audioQueue.length === 0) {
                isPlaying = false;
                return;
            }

            // Ensure AudioContext is running (browsers suspend it until user interaction)
            if (audioContext.state === 'suspended') {
                log('Resuming AudioContext...', 'event');
                await audioContext.resume();
            }

            isPlaying = true;
            const int16Data = audioQueue.shift();

            // Convert Int16 to Float32
            const float32Data = new Float32Array(int16Data.length);
            for (let i = 0; i < int16Data.length; i++) {
                float32Data[i] = int16Data[i] / 32768;
            }

            // Create audio buffer at 16kHz (the sample rate of received audio)
            // Note: AudioContext may resample internally if its sample rate differs
            const buffer = audioContext.createBuffer(1, float32Data.length, 16000);
            buffer.getChannelData(0).set(float32Data);

            log(`Playing ${int16Data.length} samples (context: ${audioContext.sampleRate}Hz, state: ${audioContext.state})`, 'audio');

            // Play
            const source = audioContext.createBufferSource();
            source.buffer = buffer;
            source.connect(audioContext.destination);
            source.onended = () => playNextChunk();
            source.start();
        }

        function stop() {
            stopRecording();

            if (ws) {
                // Disable streaming before closing
                if (ws.readyState === WebSocket.OPEN) {
                    ws.send(JSON.stringify({ type: 'set_streaming', enabled: false }));
                }
                ws.close();
                ws = null;
            }

            if (mediaStream) {
                mediaStream.getTracks().forEach(t => t.stop());
                mediaStream = null;
            }

            if (audioContext) {
                audioContext.close();
                audioContext = null;
            }

            audioQueue = [];
            isPlaying = false;

            startBtn.disabled = false;
            endTurnBtn.disabled = true;
            stopBtn.disabled = true;
            modeBadge.style.display = 'none';
            streamingIndicator.style.display = 'none';
            placeholder.style.display = 'block';
            setStatus('Disconnected');
            log('Stopped', 'event');
        }
    </script>
</body>
</html>
